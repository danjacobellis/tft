{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65434349-eb3f-472f-8eb0-51eb09fd6ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*`torch.cuda.amp.autocast.*\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from types import SimpleNamespace\n",
    "from datasets import load_dataset\n",
    "from timm.optim import Adan\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose, RandAugment, RandomErasing, Resize, RandomCrop, CenterCrop, \n",
    "    PILToTensor, ConvertImageDtype, MixUp, CutMix, ToPILImage\n",
    ")\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import copy\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a13d07b4-216e-4aeb-a71c-267a306dd8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3b02af8cad4bf8903c9a14818f9166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8861980097431ba6114c29c5d817d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b85012912004725a028e03c9babeac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c128e4cd40c4f98bc2e2e08160ce897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f064542b35f4fb4b3aa9ee411d017b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1281167 Valid samples: 50000\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('highest')\n",
    "\n",
    "########################################################\n",
    "# 1. CONFIG & HYPERPARAMS\n",
    "########################################################\n",
    "config = SimpleNamespace()\n",
    "\n",
    "# Basic training\n",
    "config.epochs = 20\n",
    "config.batch_size = 128\n",
    "config.num_workers = 8\n",
    "config.grad_accum_steps = 1\n",
    "config.max_lr = 2e-2  # single-cycle maximum LR\n",
    "config.lr_pow = 1.0\n",
    "\n",
    "# Weight decay, ignoring norm/bias\n",
    "config.weight_decay = 0.1\n",
    "config.no_wd_keys = ['norm', 'bias']\n",
    "\n",
    "# Data augmentation\n",
    "config.randaug_n = 2\n",
    "config.randaug_m = 5\n",
    "config.randerase = 0.2\n",
    "\n",
    "# Progressive resizing: 4 stages over 20 epochs => [0..4] ->128, [5..9]->160, [10..14]->192, [15..19]->224\n",
    "config.progressive_resize = True\n",
    "config.progressive_sizes = [128, 160, 192, 224]\n",
    "config.rrc = True  # If True, random resize approach (like RRC) instead of fixed resize+crop\n",
    "\n",
    "# Mixup & CutMix\n",
    "config.mixup_alpha = 0.2\n",
    "config.cutmix_alpha = 0.2\n",
    "\n",
    "# Label smoothing\n",
    "config.label_smoothing = 0.1\n",
    "\n",
    "# MESA / EMA teacher-student\n",
    "config.mesa = True\n",
    "config.mesa_ratio = 2.0\n",
    "config.mesa_start = 0.25  # fraction of training after which we add MESA loss\n",
    "\n",
    "# We'll keep 20 epochs, single-cycle LR, Adan, etc.\n",
    "seed = 42\n",
    "device = \"cuda:0\"\n",
    "\n",
    "\n",
    "########################################################\n",
    "# 2. DATASETS\n",
    "########################################################\n",
    "dataset_train = load_dataset('timm/imagenet-1k-wds', split='train')\n",
    "dataset_valid = load_dataset('timm/imagenet-1k-wds', split='validation')\n",
    "num_train = dataset_train.num_rows\n",
    "num_valid = dataset_valid.num_rows\n",
    "\n",
    "steps_per_epoch = num_train // config.batch_size\n",
    "total_steps = config.epochs * steps_per_epoch\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print(\"Train samples:\", num_train, \"Valid samples:\", num_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591c7811-9141-4597-99eb-c40b59eb66e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model param count: 52.65M\n"
     ]
    }
   ],
   "source": [
    "from timm.models.efficientvit_mit import EfficientVitLarge, GELUTanh\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create an EfficientVitLarge w/ optional dropout / drop_path.\"\"\"\n",
    "\n",
    "    class GroupNorm8(torch.nn.Module):\n",
    "        def __init__(self, num_features, eps=1e-5, affine=True):\n",
    "            super().__init__()\n",
    "            self.gn = torch.nn.GroupNorm(num_groups=8, num_channels=num_features, eps=eps, affine=affine)\n",
    "        def forward(self, x):\n",
    "            return self.gn(x)\n",
    "\n",
    "    model = EfficientVitLarge(\n",
    "        in_chans=3,\n",
    "        widths=(32, 64, 128, 256, 512),\n",
    "        depths=(1, 1, 1, 6, 6),\n",
    "        head_dim=32,\n",
    "        norm_layer=GroupNorm8,\n",
    "        act_layer=GELUTanh,\n",
    "        global_pool='avg',\n",
    "        head_widths=(3072, 3200),\n",
    "        num_classes=1000\n",
    "    )\n",
    "    # Replace final norm if needed\n",
    "    if hasattr(model, 'head') and hasattr(model.head, 'in_conv'):\n",
    "        if hasattr(model.head.in_conv, 'norm'):\n",
    "            old = model.head.in_conv.norm\n",
    "            model.head.in_conv.norm = GroupNorm8(\n",
    "                num_features=old.num_features,\n",
    "                eps=old.eps,\n",
    "                affine=old.affine\n",
    "            )\n",
    "    return model\n",
    "\n",
    "model = create_model().to(device)\n",
    "print(f\"Model param count: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3342b886-54da-455b-ba10-6a5c0cfede5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAHelper:\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.decay = decay\n",
    "        self.teacher = copy.deepcopy(model)\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, student):\n",
    "        with torch.no_grad():\n",
    "            d = self.decay\n",
    "            for tparam, sparam in zip(self.teacher.parameters(), student.parameters()):\n",
    "                tparam.copy_(tparam * d + sparam * (1 - d))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.teacher(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea60649a-e029-49f0-9860-e89e6f4da9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_groups = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        use_wd = True\n",
    "        for kw in config.no_wd_keys:\n",
    "            if kw in name.lower():\n",
    "                use_wd = False\n",
    "                break\n",
    "        wd_val = config.weight_decay if use_wd else 0.0\n",
    "        param_groups.append({'params': [p], 'weight_decay': wd_val})\n",
    "\n",
    "optimizer = Adan(param_groups, lr=1.0, weight_decay=0.0, caution=True)\n",
    "\n",
    "def one_cycle_cosine(step):\n",
    "    # Single-cycle cosine from 0 -> max_lr -> 0 over total_steps\n",
    "    t = step / (total_steps - 1)\n",
    "    t = t**config.lr_pow\n",
    "    lr = 0.5 * (1.0 - math.cos(2.0 * math.pi * t)) * config.max_lr + 1e-15\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lambda s: one_cycle_cosine(s)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "935afcfd-41b7-4462-a1a3-1a504c16ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_interpolation():\n",
    "    choices = [InterpolationMode.BILINEAR, InterpolationMode.BICUBIC, InterpolationMode.LANCZOS]\n",
    "    return random.choice(choices)\n",
    "\n",
    "def get_epoch_size(epoch):\n",
    "    if not config.progressive_resize:\n",
    "        return config.progressive_sizes[-1]\n",
    "    # For example, 5 epochs each stage:\n",
    "    stage_len = config.epochs // len(config.progressive_sizes)\n",
    "    idx = min(epoch // stage_len, len(config.progressive_sizes)-1)\n",
    "    return config.progressive_sizes[idx]\n",
    "\n",
    "\n",
    "def build_train_prebatch_transform(epoch):\n",
    "    \"\"\"\n",
    "    Returns a transform that does:\n",
    "      1) Resize or random-resized-crop\n",
    "      2) RandAug + random erasing\n",
    "      3) Convert to float => scale to [-1,1]\n",
    "\n",
    "    We do all of this per-sample (not on an entire batch at once).\n",
    "    \"\"\"\n",
    "    img_size = get_epoch_size(epoch)\n",
    "\n",
    "    # For a true \"RandomResizedCrop w/ random interpolation\" you'd typically do something like:\n",
    "    # transforms.RandomResizedCrop(img_size, scale=(0.08,1.0), ratio=(0.75,1.3333), interpolation= ...)\n",
    "    # but torchvision.transforms.v2 doesn't provide a direct \"random interpolation\" argument. We'll hack it:\n",
    "    # We'll just choose an interpolation for the entire epoch, or you could do it per-sample with a custom class.\n",
    "    interp = random_interpolation()\n",
    "\n",
    "    if config.rrc:\n",
    "        # We'll do a naive approach: resize to (img_size, img_size) for demonstration\n",
    "        # If you want the actual 'random crop of various scales', you'd do something else\n",
    "        resize_op = Resize((img_size, img_size), interpolation=interp)\n",
    "    else:\n",
    "        # normal \"resize short edge to img_size + randomcrop\"\n",
    "        # for simplicity, let's do a direct approach\n",
    "        resize_op = Compose([\n",
    "            Resize(img_size + 32, interpolation=interp),\n",
    "            RandomCrop((img_size, img_size))\n",
    "        ])\n",
    "\n",
    "    transform = Compose([\n",
    "        resize_op,\n",
    "        # Now RandAug\n",
    "        RandAugment(num_ops=config.randaug_n, magnitude=config.randaug_m),\n",
    "        # Convert to tensor float\n",
    "        PILToTensor(),\n",
    "        ConvertImageDtype(torch.float32),\n",
    "        # Random Erase\n",
    "        RandomErasing(p=config.randerase),\n",
    "        # Finally map [0,1] => [-1,1]\n",
    "        lambda x: x*2.0 - 1.0,\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def build_valid_transform():\n",
    "    # Typically, center-crop for val at the final size or something like that\n",
    "    final_size = config.progressive_sizes[-1]  # e.g. 224\n",
    "    interp = InterpolationMode.BICUBIC\n",
    "    return Compose([\n",
    "        Resize(final_size+32, interpolation=interp),\n",
    "        CenterCrop(final_size),\n",
    "        PILToTensor(),\n",
    "        ConvertImageDtype(torch.float32),\n",
    "        lambda x: x*2.0 - 1.0,\n",
    "    ])\n",
    "\n",
    "mixup_tfm = MixUp(alpha=config.mixup_alpha, num_classes=1000)\n",
    "cutmix_tfm = CutMix(alpha=config.cutmix_alpha, num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd3ba0f9-141b-4bc2-b919-1b0c8cc17ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(batch, epoch):\n",
    "    \"\"\"\n",
    "    1) For each sample => pre-batch transforms (RandAug, etc.)\n",
    "    2) Stack into (images, labels)\n",
    "       (Here labels are integer class indices.)\n",
    "    We'll do mixup/cutmix later on the entire (images, labels) batch.\n",
    "    \"\"\"\n",
    "    transform = build_train_prebatch_transform(epoch)\n",
    "    x_list, y_list = [], []\n",
    "    for sample in batch:\n",
    "        img = sample['jpg'].convert(\"RGB\")\n",
    "        cls_ = sample['cls']\n",
    "        x_ = transform(img)\n",
    "        x_list.append(x_)\n",
    "        y_list.append(cls_)\n",
    "    x = torch.stack(x_list, dim=0)  # [B,3,H,W]\n",
    "    y = torch.tensor(y_list, dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "def valid_collate_fn(batch):\n",
    "    transform = build_valid_transform()\n",
    "    x_list, y_list = [], []\n",
    "    for sample in batch:\n",
    "        img = sample['jpg'].convert(\"RGB\")\n",
    "        x_ = transform(img)\n",
    "        y_ = sample['cls']\n",
    "        x_list.append(x_)\n",
    "        y_list.append(y_)\n",
    "    x = torch.stack(x_list, dim=0)\n",
    "    y = torch.tensor(y_list, dtype=torch.long)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c6ecb-4940-4b19-9ee5-a116eb1d9fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      5.00% [1/20 23:15&lt;7:22:00 Epoch=0 | TrainLoss=6.2028 | ValAcc=0.0845]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='270' class='' max='10009' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.70% [270/10009 00:37&lt;22:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:714: UserWarning: Metadata Warning, tag 274 had too many entries: 4, expected 1\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:714: UserWarning: Metadata Warning, tag 274 had too many entries: 4, expected 1\n",
      "  warnings.warn(\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "/home/dgj335/.local/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    }
   ],
   "source": [
    "ema_helper = EMAHelper(model, decay=0.9999) if config.mesa else None\n",
    "\n",
    "train_criterion = torch.nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n",
    "val_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mb = master_bar(range(config.epochs))\n",
    "global_step = 0\n",
    "\n",
    "train_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in mb:\n",
    "    model.train()\n",
    "    if ema_helper:\n",
    "        ema_helper.teacher.eval()\n",
    "\n",
    "    # Set up DataLoader\n",
    "    def collate_fn_train(batch):\n",
    "        return train_collate_fn(batch, epoch)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn_train\n",
    "    )\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Training\n",
    "    pb = progress_bar(train_loader, parent=mb)\n",
    "    for (images, labels) in pb:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # We now apply Mixup or CutMix at the batch level via v2 transforms.\n",
    "        # We'll do a random pick approach:\n",
    "        pick = random.choice([\"mixup\", \"cutmix\", \"none\"])  # you can define probabilities if you want\n",
    "        if pick == \"mixup\" and config.mixup_alpha > 0:\n",
    "            (images, labels) = mixup_tfm((images, labels))\n",
    "        elif pick == \"cutmix\" and config.cutmix_alpha > 0:\n",
    "            (images, labels) = cutmix_tfm((images, labels))\n",
    "        # else \"none\" => do nothing\n",
    "\n",
    "        logits = model(images)\n",
    "\n",
    "        # If Mixup/CutMix was applied, 'labels' is [B, num_classes] float.\n",
    "        # CrossEntropyLoss can handle that in recent PyTorch (it'll do a “soft” cross-entropy).\n",
    "        loss = train_criterion(logits, labels)\n",
    "\n",
    "        # MESA\n",
    "        if ema_helper:\n",
    "            frac = epoch / config.epochs\n",
    "            if frac >= config.mesa_start:\n",
    "                with torch.no_grad():\n",
    "                    teacher_out = ema_helper.teacher(images)\n",
    "                # Simple approach: do BCE on the teacher vs. student probabilities\n",
    "                teacher_prob = torch.sigmoid(teacher_out)\n",
    "                student_prob = torch.sigmoid(logits)\n",
    "                mesa_loss = F.binary_cross_entropy(student_prob, teacher_prob)\n",
    "                loss = loss + config.mesa_ratio * mesa_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0, norm_type=2.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update EMA\n",
    "        if ema_helper:\n",
    "            ema_helper.update(model)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # LR schedule step\n",
    "        lr_scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "    epoch_loss /= max(num_batches, 1)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset_valid,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=valid_collate_fn\n",
    "    )\n",
    "    for (xv, yv) in progress_bar(val_loader, parent=mb):\n",
    "        xv, yv = xv.to(device), yv.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits_val = model(xv)\n",
    "        preds = logits_val.argmax(dim=1)\n",
    "        correct += (preds == yv).sum().item()\n",
    "        total += yv.size(0)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    valid_accs.append(val_acc)\n",
    "\n",
    "    mb.main_bar.comment = f\"Epoch={epoch} | TrainLoss={epoch_loss:.4f} | ValAcc={val_acc:.4f}\"\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'ema_state_dict': ema_helper.teacher.state_dict() if ema_helper else None,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_losses,\n",
    "        'valid_acc': valid_accs,\n",
    "        'config': vars(config),\n",
    "    }, f'checkpoint_epoch_{epoch}.pth')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
