{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f030f8-c93a-496b-a75c-eb00395923cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from datasets import load_dataset, Audio\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from torchvision.transforms.v2 import RandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7eed2f-5070-4b87-beda-ed01bb82d0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e701ae7bdae64bb5a23130fbd429bb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d81c1903fd44b29428c872199e6054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec462168b184a87bb33681fddbd9044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "device=\"cuda:1\"\n",
    "audioset = load_dataset(\"danjacobellis/audioset_opus_24kbps\",split='train').cast_column('opus', Audio(decode=False))\n",
    "dataset_train = audioset.select(range(1000000))\n",
    "dataset_valid = audioset.select(range(1000000,1040000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b696feaf-01ff-4d76-b562-9d8d9b905fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "# Training and optimizer config\n",
    "config.batch_size = 128\n",
    "config.steps_per_epoch = dataset_train.num_rows//config.batch_size\n",
    "config.grad_accum_steps = 1\n",
    "config.max_lr = (config.batch_size/128)*6e-4\n",
    "config.min_lr = config.max_lr/100\n",
    "config.plot_update = 64\n",
    "config.epochs = 200\n",
    "config.lr_scale = 40\n",
    "config.lr_offset = 0.25\n",
    "config.lr_pow = 6\n",
    "config.weight_decay = 0.\n",
    "config.num_workers = 24\n",
    "config.audio_len = 483840\n",
    "config.crop_size = 512*512\n",
    "\n",
    "# model config\n",
    "config.channels = 1\n",
    "config.J = 10\n",
    "config.embed_dim = 256\n",
    "config.dim_head = 32\n",
    "config.exp_ratio = 4.0\n",
    "config.classifier_num_classes = 632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2169b755-94e6-4693-9e4b-8ab1cee5b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from timm.models.maxxvit import LayerScale\n",
    "\n",
    "class RMSNormAct(torch.nn.Module):\n",
    "    def __init__(self, normalized_features):\n",
    "        super(RMSNormAct, self).__init__()\n",
    "        self.norm = torch.nn.RMSNorm(normalized_features)\n",
    "        self.act = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class InvertedResidual1D(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, sequence_dim, exp_ratio):\n",
    "        super(InvertedResidual1D, self).__init__()\n",
    "        self.exp_dim = int(in_dim * exp_ratio)\n",
    "        self.pw_exp = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_dim, self.exp_dim, kernel_size=1, stride=1, bias=False),\n",
    "            RMSNormAct((self.exp_dim, sequence_dim))\n",
    "        )\n",
    "        self.dw_mid = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(self.exp_dim, self.exp_dim, kernel_size=3, stride=1, padding=1, groups=self.exp_dim, bias=False),\n",
    "            RMSNormAct((self.exp_dim, sequence_dim))\n",
    "        )\n",
    "        self.se = torch.nn.Identity()\n",
    "        self.pw_proj = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(self.exp_dim, out_dim, kernel_size=1, stride=1, bias=False),\n",
    "            torch.nn.RMSNorm((out_dim, sequence_dim)) \n",
    "        )\n",
    "        self.dw_end = torch.nn.Identity()\n",
    "        self.layer_scale = LayerScale(out_dim)\n",
    "        self.drop_path = torch.nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x if x.shape[1] == self.pw_proj[0].out_channels else None\n",
    "        x = self.pw_exp(x)\n",
    "        x = self.dw_mid(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pw_proj(x)\n",
    "        x = self.dw_end(x)\n",
    "        x = self.layer_scale(x)\n",
    "        x = self.drop_path(x)\n",
    "        if shortcut is not None:\n",
    "            x += shortcut\n",
    "        return x\n",
    "\n",
    "class TransformerBlock1D(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, dim_feedforward, nhead):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=0.0,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = einops.rearrange(x, 'b c s -> b s c')\n",
    "        x = self.layer(x)\n",
    "        x = einops.rearrange(x, 'b s c -> b c s')\n",
    "        return x\n",
    "\n",
    "class AsCAN1D(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, sequence_dim, dim_head, exp_ratio):\n",
    "        super().__init__()\n",
    "        C=lambda:InvertedResidual1D(embed_dim, embed_dim, sequence_dim, exp_ratio)\n",
    "        T=lambda:TransformerBlock1D(embed_dim, int(exp_ratio*embed_dim), embed_dim//dim_head)\n",
    "        self.layers=torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(input_dim,embed_dim,kernel_size=1),\n",
    "            RMSNormAct((embed_dim, sequence_dim)),\n",
    "            C(),C(),C(),T(),\n",
    "            C(),C(),T(),T(),\n",
    "            C(),T(),T(),T()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class WaveletPooling1D(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, sequence_dim, wpt, num_levels):\n",
    "        super().__init__()\n",
    "        self.wpt = wpt\n",
    "        self.num_levels = num_levels\n",
    "        current_sequence_dim = sequence_dim\n",
    "        self.projection_down = torch.nn.ModuleList()\n",
    "        for _ in range(num_levels):\n",
    "            self.projection_down.append(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Conv1d(embed_dim, embed_dim // 2, kernel_size=1, padding=0),\n",
    "                    torch.nn.RMSNorm((embed_dim // 2, current_sequence_dim))\n",
    "                )\n",
    "            )\n",
    "            current_sequence_dim //= 2\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_levels):\n",
    "            x = self.projection_down[i](x)\n",
    "            x = self.wpt.analysis_one_level(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TFTClassifier1D(torch.nn.Module):\n",
    "    def __init__(self, config, wpt):\n",
    "        super().__init__()\n",
    "        self.wpt = wpt\n",
    "        self.ascan = AsCAN1D(\n",
    "            input_dim=config.channels*(2**config.J),\n",
    "            embed_dim=config.embed_dim,\n",
    "            sequence_dim=config.crop_size//(2**config.J),\n",
    "            dim_head=config.dim_head,\n",
    "            exp_ratio=config.exp_ratio\n",
    "        )\n",
    "        self.pool = WaveletPooling1D(\n",
    "            embed_dim=config.embed_dim,\n",
    "            sequence_dim=config.crop_size//(2**config.J),\n",
    "            wpt=wpt,\n",
    "            num_levels=int(np.log2(config.crop_size) - config.J)\n",
    "        )\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(config.embed_dim, config.classifier_num_classes, kernel_size=1),\n",
    "            torch.nn.Flatten()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.wpt(x)\n",
    "        x = self.ascan(x)\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32328b3e-b482-4bb6-98d2-714f12cc24e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 \t wpt\n",
      "11.771136 \t ascan\n",
      "0.328448 \t pool\n",
      "0.162424 \t classifier\n"
     ]
    }
   ],
   "source": [
    "from pytorch_wavelets import DWT1DForward\n",
    "from tft.transforms import WPT1D\n",
    "\n",
    "wt = DWT1DForward(J=1, mode='periodization', wave='bior4.4')\n",
    "wpt = WPT1D(wt,J=config.J).to(device)\n",
    "\n",
    "model = TFTClassifier1D(config,wpt).to(device)\n",
    "\n",
    "for name, module in model.named_children():\n",
    "    print(f\"{sum(p.numel() for p in module.parameters())/1e6} \\t {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24fc70cd-7595-4c92-b48b-19de388ec6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_crop = RandomCrop(\n",
    "    size=(1,config.crop_size),\n",
    "    pad_if_needed=True,\n",
    "    fill=0\n",
    ")\n",
    "def train_collate_fn(batch):\n",
    "    B = len(batch)\n",
    "    x = torch.zeros((B, config.channels, 1, config.crop_size), dtype=torch.float)\n",
    "    y = []\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        y.append(sample['label'])\n",
    "        x_raw, fs = torchaudio.load(uri = sample['opus']['bytes'],normalize=False)\n",
    "        x[i_sample,:,:,:] = rand_crop(x_raw.unsqueeze(0).unsqueeze(0))\n",
    "    return x[:,:,0,:], y\n",
    "\n",
    "def create_multi_hot_labels(batch_of_label_indices, num_classes):\n",
    "    batch_size = len(batch_of_label_indices)\n",
    "    labels = torch.zeros(batch_size, num_classes, dtype=torch.float32)\n",
    "    for i, sample_labels in enumerate(batch_of_label_indices):\n",
    "        indices = torch.tensor(sample_labels, dtype=torch.long)\n",
    "        labels[i].scatter_(0, indices, 1.0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db31422-3934-451e-b0b6-66352c921b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/200 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='7812' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/7812 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()(y,logits)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mb = master_bar(range(config.epochs))\n",
    "mb.names = ['per batch','smoothed']\n",
    "for i_epoch in mb:\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=train_collate_fn\n",
    "    )\n",
    "    pb = progress_bar(dataloader_train, parent=mb)\n",
    "    for i_batch, (x,y) in enumerate(pb):\n",
    "        y = create_multi_hot_labels(y, config.classifier_num_classes).to(device)\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(y,logits)\n",
    "        assert 1==0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
