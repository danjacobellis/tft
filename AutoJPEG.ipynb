{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3510095-2530-47e4-9be4-f7195bdc9d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-3b1def15-1f21-5622-ac8e-57509b6b0cf0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c876126c97634f3a8a50c6ff9fced4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117d1233455e4fdd9f5dfb884a393823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e175791a4aab4e07aa0947f13e09441b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-3b1def15-1f21-5622-ac8e-57509b6b0cf0\n",
    "device=\"cuda:0\"\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "from types import SimpleNamespace\n",
    "from timm.optim import Mars\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torchvision.transforms.v2 import ToPILImage, PILToTensor, CenterCrop, RandomCrop\n",
    "from tft.mdct import mdct2d, inverse_mdct2d\n",
    "from tft.utils import compand, decompand\n",
    "\n",
    "class PerChannelNoiseScale(nn.Module):\n",
    "    def __init__(self, num_channels, total_scale):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.alpha = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.min_scale = 1.0\n",
    "        self.total_scale = total_scale\n",
    "\n",
    "    def forward(self):\n",
    "        exp_alpha = torch.exp(self.alpha)\n",
    "        alpha_sum = exp_alpha.sum()\n",
    "        total_excess = (self.total_scale - self.min_scale) * self.num_channels\n",
    "        S_prime = total_excess * (exp_alpha / alpha_sum)\n",
    "        return self.min_scale + S_prime\n",
    "\n",
    "class NoiseInjectionModel(nn.Module):\n",
    "    def __init__(self, num_channels, total_scale):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.noise_scales = PerChannelNoiseScale(num_channels, total_scale)\n",
    "        self.hardtanh = nn.Hardtanh(min_val=-127.49, max_val=127.49)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            S = self.noise_scales()\n",
    "            S_half = (S / 2.0).view(1, -1, 1, 1)\n",
    "            noise = (torch.rand_like(x) * 2.0 - 1.0) * S_half\n",
    "            x = x + noise\n",
    "            x = self.hardtanh(x)\n",
    "            return x\n",
    "        else:\n",
    "            S = self.noise_scales() \n",
    "            S_view = S.view(1, -1, 1, 1)\n",
    "            x = self.hardtanh(x)\n",
    "            x = x / S_view\n",
    "            x = torch.round(x)\n",
    "            return x\n",
    "\n",
    "    def scale(self, x_int):\n",
    "        S = self.noise_scales()\n",
    "        S_view = S.view(1, -1, 1, 1)\n",
    "        x_float = x_int * S_view\n",
    "        x_float = self.hardtanh(x_float)\n",
    "        return x_float\n",
    "\n",
    "class BottleneckNoiseModel(nn.Module):\n",
    "    def __init__(self, in_channels, latent_channels, groups, total_scale):\n",
    "        super().__init__()\n",
    "        self.conv_down = nn.Conv2d(in_channels, latent_channels, kernel_size=1, padding=0, bias=False, groups=groups)\n",
    "        self.noise_injection = NoiseInjectionModel(latent_channels, total_scale=total_scale)\n",
    "        self.conv_up = nn.Conv2d(latent_channels, in_channels, kernel_size=1, padding=0, bias=False, groups=groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_down(x)\n",
    "        x = self.noise_injection(x)\n",
    "        x = self.conv_up(x)\n",
    "        return x\n",
    "\n",
    "    def scale(self, x_int):\n",
    "        x_float = self.noise_injection.scale(x_int)\n",
    "        x_float = self.conv_up(x_float)\n",
    "        return x_float\n",
    "\n",
    "\n",
    "def entropy_code(ints):\n",
    "\n",
    "    webp_bytes = []\n",
    "    for sample in ints:\n",
    "        c_groups = sample.shape[0] // 3\n",
    "        sample_webp = []\n",
    "        \n",
    "        for g in range(c_groups):\n",
    "            group = sample[g*3 : g*3+3]\n",
    "            \n",
    "            group = (group + 127).clamp_(0, 255).byte()\n",
    "            \n",
    "            img = ToPILImage()(group)\n",
    "            \n",
    "            buff = io.BytesIO()\n",
    "            img.save(buff, format='WEBP', lossless=True)\n",
    "            sample_webp.append(buff.getbuffer())\n",
    "        \n",
    "        webp_bytes.append(sample_webp)\n",
    "    \n",
    "    return webp_bytes\n",
    "\n",
    "\n",
    "def entropy_decode(webp_bytes):\n",
    "    batch_out = []\n",
    "    for sample_buffers in webp_bytes:\n",
    "        group_tensors = []\n",
    "        \n",
    "        for buff in sample_buffers:\n",
    "            with io.BytesIO(buff) as memfile:\n",
    "                img = PIL.Image.open(memfile).convert('RGB')\n",
    "\n",
    "            t = PILToTensor()(img).to(torch.int16) - 127\n",
    "            group_tensors.append(t)\n",
    "        \n",
    "        sample_tensor = torch.cat(group_tensors, dim=0)\n",
    "        sample_tensor = sample_tensor.unsqueeze(0)\n",
    "        batch_out.append(sample_tensor)\n",
    "    \n",
    "    decoded = torch.cat(batch_out, dim=0)\n",
    "    return decoded\n",
    "\n",
    "dataset = datasets.load_dataset(\"danjacobellis/LSDIR_540\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31802a5a-cddf-4ebd-9d87-2aca01836585",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "config.F = 64\n",
    "config.excess_noise = 3;\n",
    "config.latent_channels = 126; assert config.latent_channels%3 == 0\n",
    "config.groups = 1\n",
    "config.channels = int(3*config.F*config.F)\n",
    "config.max_lr = 1e-3\n",
    "config.min_lr = config.max_lr / 1e2\n",
    "config.lr_pow = 6\n",
    "config.img_size = 512\n",
    "config.batch_size = 16\n",
    "config.num_workers = 32\n",
    "config.epochs = 1\n",
    "config.total_steps = config.epochs * (dataset['train'].num_rows // config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcd33e0-b5ba-46a5-a126-b7aa9b80178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.096702 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BottleneckNoiseModel(\n",
    "    in_channels=config.channels,\n",
    "    latent_channels=config.latent_channels,\n",
    "    groups=config.groups,\n",
    "    total_scale=1+config.excess_noise\n",
    ").to(device)\n",
    "\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "\n",
    "optimizer = Mars(model.parameters(), lr=1.0, caution=True)\n",
    "\n",
    "def rc_sched(i_step, config):\n",
    "    t = i_step / config.total_steps\n",
    "    return (config.max_lr - config.min_lr) * (1 - ((np.cos(np.pi*t))**(2*config.lr_pow))) + config.min_lr\n",
    "\n",
    "schedule = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda i_step: rc_sched(i_step, config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c923fce2-de6a-4fb2-844f-ac0a5913cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return torch.cat([\n",
    "        PILToTensor()(\n",
    "            RandomCrop(\n",
    "                config.img_size + config.F,\n",
    "                padding=config.F,\n",
    "                padding_mode='reflect'\n",
    "            )(sample['image'])\n",
    "        ).unsqueeze(0) for sample in batch\n",
    "    ]).to(torch.float)/127.5 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd35e91-63fe-4268-aac3-d1bf00b1c085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='306' class='' max='5311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      5.76% [306/5311 00:33&lt;09:10 PSNR: 13.7, LR: 0.000188]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = [optimizer.param_groups[0]['lr']]\n",
    "mb = master_bar(range(config.epochs))\n",
    "losses = []\n",
    "for i_epoch in mb:\n",
    "    model.train()\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "            dataset['train'],\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "    pb = progress_bar(dataloader_train, parent=mb)\n",
    "    for i_batch, x in enumerate(pb):\n",
    "        x = x.to(device)\n",
    "\n",
    "        X = compand(mdct2d(x, 2*config.F, 2*config.F))/40\n",
    "    \n",
    "        inputs = einops.rearrange(256*X, 'b c h w u v -> b (u v c) h w')\n",
    "        outputs = einops.rearrange(\n",
    "            model(inputs),\n",
    "            'b (u v c) h w -> b c h w u v',\n",
    "            u=X.shape[4],\n",
    "            v=X.shape[5]\n",
    "        )/256\n",
    "        \n",
    "        x_hat = inverse_mdct2d(decompand(40*outputs))\n",
    "     \n",
    "        loss = torch.nn.functional.mse_loss(\n",
    "            CenterCrop(config.img_size)(x),\n",
    "            CenterCrop(config.img_size)(x_hat)\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        schedule.step()\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "        pb.comment = f\"PSNR: {-10*loss.log10().item()+6.02:.3g}, LR: {learning_rates[-1]:.3g}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7af81-d678-4c59-aaaa-3708729f5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "\n",
    "# pb = progress_bar(range(config.total_steps))\n",
    "# for i_step in pb:\n",
    "    \n",
    "#     y_padded = CenterCrop((y.shape[1]+config.F, y.shape[2]+config.F))(y)\n",
    "#     Y = compand(mdct2d(y_padded, 2*config.F, 2*config.F))/40\n",
    "\n",
    "#     inputs = einops.rearrange(256*Y, 'c h w u v -> (u v c) h w').unsqueeze(0)\n",
    "#     outputs = einops.rearrange(\n",
    "#         model(inputs).squeeze(0),\n",
    "#         '(u v c) h w -> c h w u v',\n",
    "#         u=Y.shape[3],\n",
    "#         v=Y.shape[4]\n",
    "#     )/256\n",
    "    \n",
    "#     y_hat = inverse_mdct2d(decompand(40*outputs))\n",
    "#     y_hat = CenterCrop((y.shape[1], y.shape[2]))(y_hat)\n",
    " \n",
    "#     loss = torch.nn.functional.mse_loss(y,y_hat)\n",
    "#     losses.append(loss.item())\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     schedule.step()\n",
    "\n",
    "#     pb.comment = f\"PSNR: {-10*loss.log10().item()+6.02:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4844796-fbb6-4f45-b38b-2b74f1b3491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# img = PIL.Image.open(\"kodim05.png\").crop((0,0,512,512))\n",
    "# y = PILToTensor()(img).to(torch.float) / 127.5 - 1.0\n",
    "# y = y.to(device)\n",
    "# y_padded = CenterCrop((y.shape[1]+config.F, y.shape[2]+config.F))(y)\n",
    "# Y = compand(mdct2d(y_padded, 2*config.F, 2*config.F))/40\n",
    "\n",
    "# inputs = einops.rearrange(256*Y, 'c h w u v -> (u v c) h w').unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     latent = model.noise_injection(model.conv_down(inputs))\n",
    "# webp_bytes = entropy_code(latent)\n",
    "# size_bytes = sum( sum(len(b) for b in group) for group in webp_bytes)\n",
    "# print(f\"{size_bytes/1e3} KB\")\n",
    "# print(f\"{y.numel()/size_bytes}x compression ratio\")\n",
    "# print(f\"{y.numel()/latent.numel()}x dimension reduction\")\n",
    "# latent_decoded = entropy_decode(webp_bytes).to(device)\n",
    "# outputs = model.conv_up(model.noise_injection.scale(latent_decoded))\n",
    "\n",
    "# outputs = einops.rearrange(\n",
    "#     outputs.squeeze(0),\n",
    "#     '(u v c) h w -> c h w u v',\n",
    "#     u=Y.shape[3],\n",
    "#     v=Y.shape[4]\n",
    "# )/256\n",
    "\n",
    "# y_hat = inverse_mdct2d(decompand(40*outputs))\n",
    "# y_hat = CenterCrop((y.shape[1], y.shape[2]))(y_hat)\n",
    "# y_hat = (y_hat).clamp(-1,1)\n",
    "\n",
    "# PSNR = -10*torch.nn.functional.mse_loss(y/2,y_hat/2).log10()\n",
    "# print(f\"{PSNR.item()} dB PSNR\")\n",
    "\n",
    "# ToPILImage()(y_hat/2+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3581a7-9022-4c0b-8b39-bc30245f6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.crop((0,0,512,512)).save(\"test.jpg\",quality=5)\n",
    "# jpeg = PIL.Image.open(\"test.jpg\")\n",
    "# MSE = torch.nn.functional.mse_loss(PILToTensor()(jpeg).to(torch.float)/255, y.to(\"cpu\")/2 + 0.5)\n",
    "# print(f\"{-10*MSE.log10()} PSNR\")\n",
    "# !du -sh test.jpg\n",
    "# display(jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
